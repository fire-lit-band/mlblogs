注：学习joyrl的笔记

# 马尔科夫决策过程

![img](https://johnjim0816.com/joyrl-book/figs/ch2/interaction_mdp.png)

对于决策过程，可以被简单的概括为如上的图:

智能体：做动作的主体（或者说研究的对象）

环境：对智能体产生影响的因素

奖励(return)：

状态：环境的状态

动作：根据“奖励”和“状态”智能体所作出的反应、

策略：$\pi(s|a)$表示策略函数，一般指在状态s下执行动作a的概率分布

## 目的

我们的目的，是最大化累积的奖励（也就是回报），翻译成人话就是

$G_t=r_{t+1}+r_{t+2}+\cdots+r_T$

如果时间趋近于无穷的话，此时我们就必须引入折扣因子（discount factor）$\gamma$，此时回报会变成：

$G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots=\sum_{k=0}^{T=\infty}\gamma^kr_{t+k+1}$

进一步简化，我们会有

$\begin{aligned}
G_{t}& \doteq r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\gamma^3r_{t+4}+\cdots   \\
&=r_{t+1}+\gamma\left(r_{t+2}+\gamma r_{t+3}+\gamma^2r_{t+4}+\cdots\right) \\
&=r_{t+1}+\gamma G_{t+1}
\end{aligned}$

## 五元组

五元组$<S,A,R,P,\gamma>$ 来用来描述一个马尔科夫决策链

。其中$S$ 表示状态空间，即所有状态的集合

$A$ 表示动作空间

R$ 表示奖励函数$

P$表示状态转移矩阵，

$\gamma$ 表示折扣因子。

# 动态规划

## state value function(状态价值函数)

$\begin{aligned}
V_{\pi}(s)& =\mathbb{E}_\pi[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots|S_t=s]  \\
&=\mathbb{E}_\pi[G_t|S_t=s]
\end{aligned}$

特定状态出发，按照某种策略所能得到的回报

## 动作价值函数

$Q_\pi(s,a)=\mathbb{E}_\pi\left[G_t\mid s_t=s,a_t=a\right]$

特定状态，并进行特定行为后得到的回报

## relationship

$V_\pi(s)=\sum_{a\in A}\pi(a\mid s)Q_\pi(s,a)$

# 贝尔曼方程

## 状态函数的贝尔曼方程

$\begin{aligned}
V_{\pi}(s)& =\mathbb{E}_\pi\left[G_t\mid S_t=s\right]  \\
&=\mathbb{E}_\pi\left[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots\mid S_t=s\right] \\
&=\mathbb{E}\left[R_{t+1}\mid s_t=s\right]+\gamma\mathbb{E}\left[R_{t+2}+\gamma R_{t+3}+\gamma^2R_{t+4}+\cdots\mid S_t=s\right] \\
&=R(s)+\gamma\mathbb{E}\left[G_{t+1}\mid S_t=s\right] \\
&=R(s)+\gamma\mathbb{E}\left[V_{\pi}\left(s_{t+1}\right)\mid S_{t}=s\right] \\
&=R(s)+\gamma\sum_{s^{\prime}\in S}P\left(S_{t+1}=s^{\prime}\mid S_t=s\right)V_\pi\left(s^{\prime}\right) \\
&=R(s)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s\right)V_\pi\left(s^{\prime}\right)
\end{aligned}$

实际上可以简单理解为，在状态s，让他任意走一步，到s'，此时根据全概率

## 动作价值函数

$Q_\pi(s,a)=R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)\sum_{a^{\prime}\in A}\pi\left(a^{\prime}\mid s^{\prime}\right)Q_\pi\left(s^{\prime},a^{\prime}\right)$

# 最优问题

$\begin{gathered}
V^{*}(s) =\max_a\mathbb{E}\left[R_{t+1}+\gamma V^*\left(S_{t+1}\right)\mid S_t=s,A_t=a\right] \\
=\max_a\sum_{s^{\prime},r}p(s^{\prime},r|s,a)[r+\gamma V^*(s^{\prime})] 
\end{gathered}$

$\begin{gathered}
Q^*(s,a) =\mathbb{E}\left[R_{t+1}+\gamma\max_{a^{\prime}}Q^*\left(S_{t+1},a^{\prime}\right)\mid S_t=s,A_t=a\right] \\
=\sum_{s^{\prime},r}p\left(s^{\prime},r\mid s,a\right)\left[r+\gamma\max_{a^{\prime}}Q^*\left(s^{\prime},a^{\prime}\right)\right] 
\end{gathered}$

# 策略迭代与价值迭代

[强化学习基础篇: 策略迭代 (Policy Iteration) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/34006925)

这篇文章写得还是不错的。作者说：不稳定，建议深度强化学习之前的几章不要沉迷，打打基础，后面才是重点，因此这里我就先略过了